{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a vu le cadre général en cours. On va pratiquer en jouant un peu avec les limites de ce qu'on a vu.\n",
    "* On commence avec un réseau très simple comprenant une couche de sortie sigmoide.\n",
    "* Puis on découvre un problème canonique qui justifie l'existence d'une couche cachée.\n",
    "* Puis on chargera les données MNIST, bibliothèque TensorFlow et on essaiera d'apprendre à lire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un problème trivial\n",
    "\n",
    "Considérons les données suivantes. On veut prédire $y$ à partir de $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Exercice**: Que peut-on remarquer sans aucun calcul ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Réponse :** La première variable $x_1$ prédit exactement la variable de sortie $y$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va définir un réseau de neurones avec 3 entrées, 1 neurone caché et 1 neurone de sortie. Faites un dessin en notant où se situent $X$, $Z$, $Y$, $\\alpha$ et $\\beta$ pour faciliter le raisonnement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Exercice**: Faites un dessin en notant où se situent $X$, $Z$, $Y$, $\\alpha$ et $\\beta$ pour faciliter le raisonnement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"Renvoie la fonction sigmoide évaluée en x.\"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    \"\"\"Renvoie la dérivée de la fonction sigmoide en x.\"\"\"\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y*(1-y)\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "alpha = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le cours s'est concentré sur l'application de la rétro-propagation *exemple par exemple* en calculant les $R_i$ et les $\\frac{\\partial R_i}{\\partial \\alpha}$ un par un. <br>\n",
    "Ici, on dispose de toute la base d'apprentissage d'un coup, on peut donc calculer $R$ et $\\frac{\\partial R}{\\partial \\alpha}$ en une passe.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Exercice**: Reprenez les équations du cours pour effectuer le calcul de mise à jour des poids au format *batch*.\n",
    "</div>\n",
    "<div class=\"alert alert-warning\">\n",
    "**Exercice**: Ecrivez l'algorithme de rétro-propagation avec un pas de descente de 1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EMPTY CELL FOR STUDENTS\n",
    "nb_steps = 10000\n",
    "error = np.zeros(nb_steps)\n",
    "for t in range(nb_steps):\n",
    "    # forward propagation\n",
    "    Zinput = np.dot(X,alpha)\n",
    "    Z = sigmoid(Zinput)\n",
    "    Y = Z\n",
    "\n",
    "    # error backpropagation\n",
    "    Y_error = y - Y\n",
    "    error[t] = np.linalg.norm(Y_error)\n",
    "    delta = -2.*Y_error \n",
    "    s = sigmoid_der(Zinput)*delta\n",
    "\n",
    "    # update weights dR/dalpha = s X^T\n",
    "    alpha -= np.dot(X.T,s)\n",
    "print(\"Output After Training:\")\n",
    "print(Y)\n",
    "print(\"Weights (alpha):\")\n",
    "print(alpha)\n",
    "print(\"Error:\")\n",
    "print(error[-1])\n",
    "plt.plot(error)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le problème XOR\n",
    "\n",
    "Un problème un peu plus difficile maintenant. Observons les données ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Exercice**: La corrélation entre $x_1$ et $y$ n'est plus aussi évidente. Que peut-on remarquer cependant ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Réponse :** $y$ = XOR($x_1$, $x_2$)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le réseau précédent à un neurone caché et un neurone de sortie est équivalent, en fait, à un réseau sans couche cachée et dont on utilise la fonction sigmoïde pour ramener la combinaison linéaire des $x_i$ entre zéro et un. Il ne peut donc approcher correctement la sortie que si on peut approximativement estimer cette dernière avec une combinaison linéaire des entrées. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Exercice**: Qu'en déduisez-vous sur ce qu'on peut attendre du réseau précédent sur notre nouveau problème ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Réponse :** Une combinaison linéaire qui approche correctement la fonction XOR n'existe pas. On va avoir besoin d'une architecture d'approximation plus riche.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour se donner un peu plus de pouvoir de représentation, on va se donner 4 neurones sur la couche cachée de façon à pouvoir représenter les quatre combinaisons de $x_1$ et $x_2$ (dont on suppose qu'elles sous-tendent notre problème).\n",
    "Comme on sait également que notre sortie est entre zéro et un, on va passer une fonction sigmoïde sur la sortie, donc $Y_k=g_k(T) = \\texttt{sigmoid}(T_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Exercice**: A vous d'écrire l'apprentissage des poids de ce réseau !\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EMPTY CELL FOR STUDENTS\n",
    "# Il y a un bug dans ma correction...\n",
    "alpha = 2*np.random.random((3,4)) - 1\n",
    "beta = 2*np.random.random((4,1))-1\n",
    "\n",
    "nb_steps = 100000\n",
    "error = np.zeros(nb_steps)\n",
    "for t in range(nb_steps):\n",
    "    # forward propagation\n",
    "    Zinput = np.dot(X,alpha)\n",
    "    Z = sigmoid(Zinput) # lines=examples, columns=neurons\n",
    "    Yinput = np.dot(Z,beta)\n",
    "    Y = sigmoid(Yinput) # lines=examples\n",
    "\n",
    "    # error backpropagation\n",
    "    Y_error = y - Y\n",
    "    error[t] = np.linalg.norm(Y_error)\n",
    "    delta = -2*Y_error*sigmoid_der(Yinput) # lines=examples\n",
    "    s = sigmoid_der(Zinput)*delta # lines=examples, columns=neurons\n",
    "\n",
    "    # update weights dR/dbeta = delta Z^T  dR/dalpha = s X^T\n",
    "    beta -= np.dot(Z.T,delta)\n",
    "    alpha -= np.dot(X.T,s)\n",
    "print(\"Output After Training:\")\n",
    "print(Y)\n",
    "print(\"Weights:\")\n",
    "print(alpha)\n",
    "print(beta)\n",
    "print(\"Error:\")\n",
    "print(error[-1])\n",
    "plt.plot(error)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caractères manuscrits\n",
    "\n",
    "On va travailler avec la célèbre base d'apprentissage [MNIST](https://en.wikipedia.org/wiki/MNIST_database) comprenant des chiffres manuscrits extraits de différentes sources. Le but est de construire un classifieur indiquant la valeur du chiffre inscrit dans chaque imagette.<br>\n",
    "On va profiter de cet exercice pour s'initier à l'usage de la bibliothèque [TensorFlow](https://www.tensorflow.org).<br>\n",
    "<br>\n",
    "Ce qu'il faut savoir pour démarrer avec TensorFlow :\n",
    "<ul>\n",
    "<li> Les dimensions des données d'entrée et de sortie d'un calcul sont définies via des `tf.placeholder`.\n",
    "<li> Les paramètres manipulées dans le programme sont appelés `tf.Variables` et sont représentées sous formes de tableaux multidimensionnels appelés tenseurs.\n",
    "<li> Les opérations sur les `tf.Variables` forment les noeuds d'un graphe de calcul. Ces opérations correspondent pour nous au flux des opérations effectuées lors d'une traversée du réseau de neurones et de sa mise à jour.\n",
    "<li> L'exécution d'un graphe de calculs se fait au sein d'une session.\n",
    "<li> La méthode `run(x)` d'une session permet d'exectuer l'opération `x` ou de récupérer la valeur de la variable `x`.\n",
    "</ul>\n",
    "<br>\n",
    "Un petit exemple pour démarrer doucement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# On crée une variable initialisée à 0\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# On crée une opération qui ajoute 1 à la variable.\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# Les variables doivent être initialisées. \n",
    "# Avant d'exécuter d'autres opérations il faut donc \n",
    "# ajouter cette opération d'initialisation au graphe.\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# On crée la session et exécute les opérations.\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "# run(state) permet de récupérer la valeur de la variable.\n",
    "print(sess.run(state))\n",
    "# run(update) permet d'exécuter l'opération update\n",
    "for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))\n",
    "\n",
    "# On ferme la session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De façon pratique, TensorFlow fournit plusieurs bases d'apprentissage standard. On charge les données MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)\n",
    "print(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Chaque image est un carré de 28x28 pixels en noir et blanc (784 valeurs entre 0 et 1) et on dispose de 55000 exemples d'apprentissage.<br>\n",
    "On commence par déclarer à TensorFlow quelles seront les entrées et sorties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va à présent considérer une unique couche cachée dans notre réseaux, comportant $M=15$ neurones. Pour cela, on va déclarer les poids du réseau comme des `tf.variables` (donc l'état de notre réseau à un moment donné est fourni par les valeurs de toutes ces variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 15\n",
    "w1 = tf.Variable(tf.truncated_normal([784, M])) # alpha\n",
    "b1 = tf.Variable(tf.truncated_normal([1, M])) # alpha_0\n",
    "w2 = tf.Variable(tf.truncated_normal([M, 10])) # beta\n",
    "b2 = tf.Variable(tf.truncated_normal([1, 10])) # beta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow fournit un certain nombre de fonctions d'activation, dont la fonction sigmoïde. On aurait pu réutiliser celle plus haut, ou encore la reconstruire à partir des blocs de base de TensorFlow (tf.div, tf.add, tf.constant, tf.exp, etc.) mais pourquoi ne pas utiliser la fonction directement fournie ?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward pass\n",
    "Zinput  = tf.add(tf.matmul(x, w1), b1)\n",
    "Z       = tf.sigmoid(Zinput)\n",
    "Yinput  = tf.add(tf.matmul(Z, w2), b2)\n",
    "Y       = tf.sigmoid(Yinput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow implémente une propriété très utile : la différentiation automatique. Cela permet à la bibliothèque d'effectuer directement les calculs de gradients. Ainsi, la rétro-propagation s'écrit simplement en écrivant la fonction `cost` puis en créant comme opération `step` une descente de gradient à pas donné (0.5 ici) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sum_11:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "Y_error = tf.sub(Y,y)\n",
    "cost = tf.reduce_sum(tf.mul(Y_error, Y_error))\n",
    "step = tf.train.GradientDescentOptimizer(0.5).minimize(cost)\n",
    "print(cost)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(step, feed_dict = {x: mnist.train.images, y : mnist.train.labels})\n",
    "\n",
    "error = np.zeros(nb_steps)\n",
    "for t in range(nb_steps):\n",
    "    sess.run(step, feed_dict = {x: mnist.train.images, y : mnist.train.labels})\n",
    "    #Previously, error[t] = sess.run(cost)\n",
    "    # En general pour eviter des erreurs de mémoire, on le decompose en batch du type\n",
    "    # error[t] = sess.run(cost, feed_dict = {x: mnist.train.images[:50,:,:,:], y : mnist.train.labels[:50,:]})\n",
    "    # A verifier mais je sais pas si la il ne fait une descente exacte de gradient...\n",
    "    error[t] = sess.run(cost, feed_dict = {x: mnist.train.images, y : mnist.train.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histoire de se faire la main avec TensorFlow, on va ignorer la possibilité ci-dessus et définir les opérations de mise à jour des poids du réseau nous-mêmes.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Exercice**: dans le cours on s'était débarassés de $b_1$ et $b_2$ pour simplifier les notations. Que valent $\\frac{\\partial R}{\\partial b_2}$ et $\\frac{\\partial R}{\\partial b_1}$ ici ?<br>\n",
    "On se resservira de ce résultat dans la prochaine cellule.\n",
    "</div>\n",
    "\n",
    "Pour l'exercice, on va reconstruire la dérivée de la fonction sigmoïde à partir de la fonction sigmoïde elle-même.\n",
    "On peut ainsi définir le flux d'opérations correspondant aux paramètres de notre réseau et à la rétropropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfsigmoid_der(x):\n",
    "    \"\"\"Calcule la dérivée de la fonction sigmoide en x (utilise les blocs de base de tensorflow)\"\"\"\n",
    "    return tf.mul(tf.sigmoid(x), tf.sub(tf.constant(1.0), tf.sigmoid(x)))\n",
    "\n",
    "# back propagation\n",
    "delta   = tf.mul(tf.constant(-2.), tf.mul(Y_error, tfsigmoid_der(Yinput)))\n",
    "d_w2 = tf.matmul(tf.transpose(Z), delta)\n",
    "d_b2 = tf.reduce_mean(delta,0) # Erreur de dimension delta=(55000,10) d_b2=(1,10): j'ai mis une moyenne parce qu'il fallait que j'y aille mais il faut mettre la vrai formule =)\n",
    "s = tf.mul(tfsigmoid_der(Zinput), tf.matmul(delta, tf.transpose(w2)))\n",
    "d_w1 = tf.matmul(tf.transpose(x), s) # Idem\n",
    "d_b1 = tf.reduce_mean(s,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble des opérations définissant les calculs étant définies, on peut enfin définir l'opération de mise à jour des poids du réseau. Par souci de lisibilité, TensorFlow permet de passer à la fonction `run()` une liste d'opérations à exécuter ce que l'on fait ci-dessous avec la liste `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = tf.constant(0.5)\n",
    "step = [\n",
    "    tf.assign(w1, tf.sub(w1, tf.mul(gamma, d_w1))),\n",
    "    tf.assign(b1, tf.sub(b1, tf.mul(gamma, d_b1))),\n",
    "    tf.assign(w2, tf.sub(w2, tf.mul(gamma, d_w2))),\n",
    "    tf.assign(b2, tf.sub(b2, tf.mul(gamma, d_b2)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7feeca1bfe90>> ignored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.39105713,  0.9464646 , -1.6174078 , ...,  0.20392275,\n",
       "         -0.84285444, -0.19466877],\n",
       "        [-1.18967962,  1.53323662, -0.82223296, ...,  0.53770059,\n",
       "         -0.48112175, -1.74261248],\n",
       "        [-0.39739576, -1.16136229,  0.60007823, ..., -0.39757511,\n",
       "          0.55096483,  0.63800848],\n",
       "        ..., \n",
       "        [ 0.4417989 ,  0.16537434,  1.46516907, ...,  1.53321171,\n",
       "          0.8441987 , -1.55418003],\n",
       "        [-1.40307224,  1.89231539, -0.03533609, ...,  0.80907506,\n",
       "         -1.23361552,  1.73714483],\n",
       "        [ 1.26243794, -0.00199339,  0.90219289, ..., -0.18337925,\n",
       "         -0.5475055 , -0.19386891]], dtype=float32),\n",
       " array([[ 1.57779908,  0.28157753, -1.17512202, -0.95733321, -1.30810094,\n",
       "         -0.03279183, -0.21226141, -0.74065417, -0.19090405, -1.89197791,\n",
       "         -0.05389735, -1.94149005,  0.80892378,  1.0173471 ,  0.71864992]], dtype=float32),\n",
       " array([[ 3087.51318359,  1325.39465332,   511.47457886,  1606.13781738,\n",
       "          1253.70556641,  2724.30859375,  2401.13696289,  2012.98120117,\n",
       "           818.76062012,  1634.28820801],\n",
       "        [ 3405.08837891,  2175.71728516,  1284.4119873 ,  1435.22009277,\n",
       "          1538.13037109,  2580.86547852,  3262.44433594,  2771.03662109,\n",
       "          1024.16186523,  1483.7175293 ],\n",
       "        [ 3720.57519531,  2576.1730957 ,  1050.9552002 ,  1811.84936523,\n",
       "          1473.63244629,  3006.38110352,  2958.87597656,  2573.18798828,\n",
       "          1796.47900391,  2543.93579102],\n",
       "        [  381.17269897,   425.03970337,    59.63597488,   350.17886353,\n",
       "           348.96365356,   364.95928955,   106.72502899,   435.95352173,\n",
       "           221.1305542 ,   328.37677002],\n",
       "        [ 1848.4095459 ,  1145.08911133,   861.10162354,  1094.66442871,\n",
       "           964.23895264,  1644.69677734,  1613.93041992,  1288.93493652,\n",
       "           927.41479492,  2041.45495605],\n",
       "        [ 1803.8515625 ,  1127.74060059,   699.6060791 ,  1135.44873047,\n",
       "           438.67166138,  1621.46435547,  1788.19177246,  1607.84570312,\n",
       "           830.63153076,  1600.10705566],\n",
       "        [ 1768.27722168,  1446.31970215,  1607.1907959 ,   937.11395264,\n",
       "           709.16369629,  1191.75146484,  1912.71826172,  1571.63684082,\n",
       "           955.36639404,  2021.04553223],\n",
       "        [  843.7388916 ,   612.84411621,   125.74051666,   747.03491211,\n",
       "           297.42440796,   471.06912231,   813.29650879,   532.65753174,\n",
       "           277.43792725,   876.03588867],\n",
       "        [ 4776.82470703,  3112.88134766,  1722.71704102,  2443.1875    ,\n",
       "          2100.83813477,  3819.8125    ,  3906.89355469,  3720.74072266,\n",
       "          2091.72216797,  3223.42749023],\n",
       "        [ 4166.92138672,  2657.98022461,  1747.33105469,  2119.90454102,\n",
       "          1851.56066895,  3266.8762207 ,  3231.40185547,  3442.67700195,\n",
       "          2008.52734375,  2622.4074707 ],\n",
       "        [ 2348.30078125,  1438.6895752 ,   933.08056641,  1224.42248535,\n",
       "          1150.01196289,  2211.81445312,  1317.09277344,  2211.59130859,\n",
       "          1283.72705078,  1119.78393555],\n",
       "        [  762.36175537,   564.94610596,   241.20770264,   410.70465088,\n",
       "           272.68026733,   385.96240234,   664.59075928,   556.34503174,\n",
       "           152.55665588,   380.74984741],\n",
       "        [ 1401.8067627 ,  1187.58093262,   527.64678955,   559.49737549,\n",
       "           925.95532227,   973.53570557,  1413.93469238,  1254.80810547,\n",
       "           872.06835938,  1217.30444336],\n",
       "        [ 4120.33642578,  2530.26806641,   905.28723145,  1892.10632324,\n",
       "          1860.50366211,  3220.25561523,  2813.91601562,  2709.65917969,\n",
       "          1503.01452637,  2225.35058594],\n",
       "        [ 3524.58227539,  2271.93676758,   992.4942627 ,  1526.22290039,\n",
       "          1441.24951172,  3213.79516602,  3077.15380859,  2651.98632812,\n",
       "          1552.28845215,  2370.75488281]], dtype=float32),\n",
       " array([[ 1.88738525,  0.78796345, -0.46041059, -0.57436454, -0.71778268,\n",
       "         -0.79013455, -0.42767227, -0.12468864,  1.99102616,  0.32268411]], dtype=float32)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(step, feed_dict = {x: mnist.train.images, y: mnist.train.labels})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
